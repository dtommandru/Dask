{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "-----------------------------\n",
    "## Prequel\n",
    "<ul>\n",
    "    <li>Introduction to Generators & Decorators</li>\n",
    "</ul>\n",
    "<h2>Dask</h2>\n",
    "\n",
    "<ul>\n",
    "    <li> Parallelizing the traditional pandas pipeline</li>\n",
    "    <li> Dask Arrays</li>\n",
    "    <li> Dask dataframes</li>\n",
    "    <li> What's there and What's not?</li>\n",
    "    <li> Dask Distributed </li>\n",
    "</ul>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generator\n",
    "\"\"\"\n",
    "If a function contains at least one yield statement (it may contain other yield or return statements), \n",
    "it becomes a generator function. Both \"yield\" and \"return\" will return some value from a function.\n",
    "\n",
    "Here is how a generator function differs from a normal function.\n",
    "\n",
    "--Generator function contains one or more yield statement.\n",
    "--When called, it returns an object (iterator) but does not start execution immediately.\n",
    "--Methods like __iter__() and __next__() are implemented automatically. So we can iterate through the items using next().\n",
    "--Once the function yields, the function is paused and the control is transferred to the caller.\n",
    "--Local variables and their states are remembered between successive calls.\n",
    "--Finally, when the function terminates, StopIteration is raised automatically on further calls.\n",
    "\n",
    "\n",
    "****The __iter__() function returns an iterator for the given object (array, set, tuple etc. or custom objects).\n",
    "It creates an object that can be accessed one element at a time using __next__() function, \n",
    "which generally comes in handy when dealing with loops.\n",
    "\"\"\"\n",
    "def my_gen():\n",
    "    n = 1\n",
    "    print('This is printed first')\n",
    "    yield n\n",
    "\n",
    "    n += 1\n",
    "    print('This is printed second')\n",
    "    yield n\n",
    "\n",
    "    n += 1\n",
    "    print('This is printed at last')\n",
    "    yield n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = my_gen()\n",
    "print(type(gen))\n",
    "#next(gen)\n",
    "#next(gen)\n",
    "#next(gen)\n",
    "#next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is how we create generators on the fly\n",
    "\"\"\"\n",
    "my_list =[1,2,3,4,5,6]\n",
    "gen = (x**2 for x in my_list)\n",
    "print(type(gen))\n",
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generator expression can be used inside functions. \n",
    "When used in such a way, the round parentheses can be dropped.\n",
    "\"\"\"\n",
    "print(sum(x**2 for x in my_list))\n",
    "print(max(x**2 for x in my_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python has an interesting feature called decorators to add functionality to an existing code.\n",
    "This is also called metaprogramming as a part of the program tries to modify another part of the program at compile time.\n",
    "\n",
    "Functions can be passed as arguments to another function.\n",
    "\n",
    "Such function that take other functions as arguments are also called higher order functions. \n",
    "Here is an example of such a function.\n",
    "\"\"\"\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    return x - 1\n",
    "\n",
    "def operate(func, x):\n",
    "    result = func(x)\n",
    "    return result\n",
    "\n",
    "print(operate(inc,3))\n",
    "print(operate(dec,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Furthermore, a function can return another function.\n",
    "\n",
    "Here, is_returned() is a nested function which is defined and returned, each time we call is_called().\n",
    "\"\"\"\n",
    "def is_called():\n",
    "    def is_returned():\n",
    "        print(\"Hello\")\n",
    "    return is_returned\n",
    "\n",
    "new = is_called()\n",
    "#Outputs \"Hello\"\n",
    "new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretty(func):\n",
    "    def inner():\n",
    "        print(\"I got decorated\")\n",
    "        func()\n",
    "    return inner\n",
    "\n",
    "def ordinary():\n",
    "    print(\"I am ordinary\")\n",
    "    \n",
    "ordinary()\n",
    "# let's decorate this ordinary function\n",
    "pretty = make_pretty(ordinary)\n",
    "pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_pretty\n",
    "def ordinary():\n",
    "    print(\"I am ordinary\")\n",
    "\n",
    "#ordinary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def addition(x, y):\n",
    "    sleep(1)\n",
    "    return x + y\n",
    "\n",
    "@delayed\n",
    "def increament(x):\n",
    "    return x+1\n",
    "\n",
    "@delayed\n",
    "def double(x):\n",
    "    return 2*x\n",
    "\n",
    "@delayed\n",
    "def add(x,y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = delayed(inc)(1)\n",
    "y = delayed(inc)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in range(1,6)]\n",
    "output = []\n",
    "\n",
    "for i in data:\n",
    "    a = increament(i)\n",
    "    b = double(i)\n",
    "    c = add(a,b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n",
    "\n",
    "print(total.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used the delayed annotation to show that we want these functions to operate lazily - to save the set of inputs and execute only on demand. `dask.delayed` is also a function which can do this, without the annotation, leaving the original function unchanged, e.g., \n",
    "```python\n",
    "    delayed_inc = delayed(inc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "### Why Dask\n",
    "Aside from the [detailed introduction](http://dask.pydata.org/en/latest/), we can summarize the basics of Dask as follows:\n",
    "- process data that doesn't fit into memory by breaking it into blocks and specifying task chains\n",
    "- parallelize execution of tasks across cores and even nodes of a cluster\n",
    "- move computation to the data rather than the other way around, to minimize communication overheads\n",
    "\n",
    "All of this allows you to get the most out of your computation resources, but program in a way that is very familiar: for-loops to build basic tasks, Python iterators, and the Numpy (array) and Pandas (dataframe) functions for multi-dimensional or tabular data, respectively.\n",
    "\n",
    "The remainder of this notebook will take you through the first of these programming paradigms. This is more detail than some users will want, who can skip ahead to the iterator, array and dataframe sections; but there will be some data processing tasks that don't easily fit into those abstractions and need to fall back to the methods here.\n",
    "\n",
    "We include a few examples at the end of the notebooks showing that the ideas behind how Dask is built are not actually that novel, and experienced programmers will have met parts of the design in other situations before. Those examples are left for the interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sys import getsizeof\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import animation\n",
    "from matplotlib import cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"./flightdelays/flightdelays-2016-{:d}.csv\"\n",
    "filenames = [template.format(i) for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_delayed(df):\n",
    "    # Compute number of delayed flights: n_delayed\n",
    "    n_delayed = (df['DEP_DELAY']>0).sum()\n",
    "    # Return percentage of delayed flights\n",
    "    return n_delayed*100/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = (pd.read_csv(file) for file in filenames)\n",
    "monthly_delayed = [pct_delayed(df) for df in dataframes]\n",
    "print(getsizeof(dataframes))\n",
    "x = range(1,6)\n",
    "plt.plot(x, monthly_delayed, marker='o', linewidth=0)\n",
    "plt.ylabel('% Delayed')\n",
    "plt.xlabel('Month - 2016')\n",
    "plt.xlim((1,6))\n",
    "plt.ylim((0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def pct_delayed(df):\n",
    "    # Compute number of delayed flights: n_delayed\n",
    "    n_delayed = df.DEP_DELAY >0\n",
    "    n_delayed = sum(n_delayed)\n",
    "    # Return percentage of delayed flights\n",
    "    return n_delayed*100/len(df)\n",
    "\n",
    "@delayed\n",
    "def read_file(fname):\n",
    "    return pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed = [pct_delayed(read_file(fname)) for fname in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delayed = sum(delayed)\n",
    "total_delayed.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Arrays\n",
    "\n",
    "Dask array provides a parallel, larger-than-memory, n-dimensional array using blocked algorithms. Simply put: distributed Numpy.\n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Documentation](http://dask.readthedocs.io/en/latest/array.html)\n",
    "* [API reference](http://dask.readthedocs.io/en/latest/array-api.html)\n",
    "\n",
    "### Blocked Algorithms* :\n",
    "A *blocked algorithm* executes on a large dataset by breaking it up into many small blocks.\n",
    "\n",
    "For example, consider taking the sum of a billion numbers.  We might instead break up the array into 1,000 chunks, each of size 1,000,000, take the sum of each chunk, and then take the sum of the intermediate sums.\n",
    "\n",
    "We achieve the intended result (one sum on one billion numbers) by performing many smaller results (one thousand sums on one million numbers each, followed by another sum of a thousand numbers.)\n",
    "\n",
    "\n",
    "You can create a `dask.array` `Array` object with the `da.from_array` function.  This function accepts\n",
    "\n",
    "1.  `data`: Any object that supports NumPy slicing.\n",
    "2.  `chunks`: A chunk size to tell us how to block up our array, like `(1000,1000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000),   # 400 million element array \n",
    "                              chunks=(1000, 1000))   # Cut into 1000x1000 sized chunks\n",
    "y = x.mean(axis=0)[::100]                            # Perform NumPy-style operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.nbytes / 1e9  # Gigabytes of the input processed lazily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute()     # Time to compute the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(20000)\n",
    "dask_data = da.from_array(data, chunks=len(data)//4)\n",
    "print(dask_data.chunks)\n",
    "print(dask_data.mean().compute())\n",
    "print(dask_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile  = './nyctaxitrip/train.csv'\n",
    "df = dd.read_csv(datafile, parse_dates=[2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = list(df.pickup_longitude.values.compute()) + list(df.dropoff_longitude.values.compute())\n",
    "latitude = list(df.pickup_latitude.values.compute()) + list(df.dropoff_latitude.values.compute())\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(longitude,latitude,'.', alpha = 0.4, markersize = 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing far places\n",
    "xlim = [-74.03, -73.77]\n",
    "ylim = [40.63, 40.85]\n",
    "df = df[(df.pickup_longitude> xlim[0]) & (df.pickup_longitude < xlim[1])]\n",
    "df = df[(df.dropoff_longitude> xlim[0]) & (df.dropoff_longitude < xlim[1])]\n",
    "df = df[(df.pickup_latitude> ylim[0]) & (df.pickup_latitude < ylim[1])]\n",
    "df = df[(df.dropoff_latitude> ylim[0]) & (df.dropoff_latitude < ylim[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = list(df.pickup_longitude.values.compute()) + list(df.dropoff_longitude.values.compute())\n",
    "latitude = list(df.pickup_latitude.values.compute()) + list(df.dropoff_latitude.values.compute())\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(longitude,latitude,'.', alpha = 0.4, markersize = 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "df['pickup_weekday'] = df['pickup_datetime'].dt.weekday\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "\n",
    "df['drop_day'] = df['dropoff_datetime'].dt.day\n",
    "df['drop_month'] = df['dropoff_datetime'].dt.month\n",
    "df['drop_weekday'] = df['dropoff_datetime'].dt.weekday\n",
    "df['drop_hour'] = df['dropoff_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Functions\n",
    "* Trivially parallelizable operations (fast):\n",
    "    *  Elementwise operations:  ``df.x + df.y``\n",
    "    *  Row-wise selections:  ``df[df.x > 0]``\n",
    "    *  Loc:  ``df.loc[4.0:10.5]``\n",
    "    *  Common aggregations:  ``df.x.max()``\n",
    "    *  Is in:  ``df[df.x.isin([1, 2, 3])]``\n",
    "    *  Datetime/string accessors:  ``df.timestamp.month``\n",
    "* Cleverly parallelizable operations (also fast):\n",
    "    *  groupby-aggregate (with common aggregations): ``df.groupby(df.x).y.max()``\n",
    "    *  value_counts:  ``df.x.value_counts``\n",
    "    *  Drop duplicates:  ``df.x.drop_duplicates()``\n",
    "    *  Join on index:  ``dd.merge(df1, df2, left_index=True, right_index=True)``\n",
    "* Operations requiring a shuffle (slow-ish, unless on index)\n",
    "    *  Set index:  ``df.set_index(df.x)``\n",
    "    *  groupby-apply (with anything):  ``df.groupby(df.x).apply(myfunc)``\n",
    "    *  Join not on the index:  ``pd.merge(df1, df2, on='name')``\n",
    "* Ingest operations\n",
    "    *  Files: ``dd.read_csv, dd.read_parquet, dd.read_json, dd.read_orc``, etc.\n",
    "    *  Pandas: ``dd.from_pandas``\n",
    "    *  Anything supporting numpy slicing: ``dd.from_array``\n",
    "    *  From any set of functions creating sub dataframes via ``dd.from_delayed``.\n",
    "    *  Dask.bag: ``mybag.to_dataframe(columns=[...])``\n",
    "\n",
    "## What doesn't work\n",
    "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
    "\n",
    "Additionally, some important operations like ``set_index`` work, but are slower\n",
    "than in Pandas because they include substantial shuffling of data, and may write out to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Distributed\n",
    "\n",
    "So far we have been calling `thing.compute()` or `dask.compute(thing)` without worrying what this entails. Now we will discuss the options available for that execution, and in particular, the distributed scheduler, which comes with additional functionality.\n",
    "\n",
    "Dask comes with four available schedulers:\n",
    "- \"threaded\": a scheduler backed by a thread pool\n",
    "- \"processes\": a scheduler backed by a process pool\n",
    "- \"single-threaded\" (aka \"sync\"): a synchronous scheduler, good for debugging\n",
    "- distributed: a distributed scheduler for executing graphs on multiple machines, see below.\n",
    "\n",
    "To select one of these for computation, you can specify at the time of asking for a result, e.g.,\n",
    "```python\n",
    "myvalue.compute(scheduler=\"single-threaded\")  # for debugging\n",
    "```\n",
    "\n",
    "or set the current default, either temporarily or globally\n",
    "```python\n",
    "with dask.config.set(scheduler='processes'):\n",
    "    # set temporarily fo this block only\n",
    "    myvalue.compute()\n",
    "\n",
    "dask.config.set(scheduler='processes')\n",
    "# set until further notice\n",
    "```\n",
    "for cluster setup :https://distributed.dask.org/en/latest/setup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = da.arange(100000,chunks= 4)\n",
    "\n",
    "import time\n",
    "for sch in ['threading', 'processes', 'sync']:\n",
    "    t0 = time.time()\n",
    "    _ = data.mean().compute(scheduler=sch)\n",
    "    print(sch, time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()  # set up local cluster on your laptop\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    time.sleep(5)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    time.sleep(3)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(7)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = delayed(inc)(1)\n",
    "y = delayed(dec)(2)\n",
    "total = delayed(add)(x, y)\n",
    "total.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
